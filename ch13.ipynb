{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Copyright (c) 2015, 2016 [Sebastian Raschka](sebastianraschka.com)\n",
    "[Li-Yi Wei](http://www.liyiwei.org)\n",
    "\n",
    "https://github.com/1iyiwei/pyml\n",
    "\n",
    "[MIT License](https://github.com/1iyiwei/pyml/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Python Machine Learning - Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 13 - Parallelizing Neural Network Training with Theano\n",
    "\n",
    "We have seen how to write a multi-layer perceptron from scratch.\n",
    "\n",
    "We can also just use existing libraries.\n",
    "* Theano, Torch, TensorFlow, Caffe, etc.\n",
    "\n",
    "Advantages of Theano\n",
    "* Python interface\n",
    "* Platform support\n",
    "* GPU support for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29448e32ee42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext watermark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"watermark -a '' -u -d -v -p numpy,matplotlib,theano,keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mwatermark\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/watermark/watermark.py\u001b[0m in \u001b[0;36mwatermark\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pyversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sysinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/watermark/watermark.py\u001b[0m in \u001b[0;36m_get_packages\u001b[0;34m(self, pkgs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                               DeprecationWarning)\n\u001b[1;32m    144\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mimported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctc_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,matplotlib,theano,keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https://github.com/rasbt/watermark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "- [Building, compiling, and running expressions with Theano](#Building,-compiling,-and-running-expressions-with-Theano)\n",
    "  - [What is Theano?](#What-is-Theano?)\n",
    "  - [First steps with Theano](#First-steps-with-Theano)\n",
    "  - [Configuring Theano](#Configuring-Theano)\n",
    "  - [Working with array structures](#Working-with-array-structures)\n",
    "  - [Wrapping things up – a linear regression example](#Wrapping-things-up:-A--linear-regression-example)\n",
    "- [Choosing activation functions for feedforward neural networks](#Choosing-activation-functions-for-feedforward-neural-networks)\n",
    "  - [Logistic function recap](#Logistic-function-recap)\n",
    "  - [Estimating probabilities in multi-class classification via the softmax function](#Estimating-probabilities-in-multi-class-classification-via-the-softmax-function)\n",
    "  - [Broadening the output spectrum by using a hyperbolic tangent](#Broadening-the-output-spectrum-by-using-a-hyperbolic-tangent)\n",
    "- [Training neural networks efficiently using Keras](#Training-neural-networks-efficiently-using-Keras)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building, compiling, and running expressions with Theano\n",
    "\n",
    "Developed by the LISA lab lead by Joshua Bengio started in 2008.\n",
    "\n",
    "Harness multi/many-core CPU/GPU without the burden of \n",
    "* parallel computing code\n",
    "* memory management across processors\n",
    "\n",
    "<img src='./images/13_01.png' width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Theano?\n",
    "\n",
    "A machine learning library with interface in Python.\n",
    "* with more speed/memory optimization\n",
    "\n",
    "Focus on tensors as the core data structure.\n",
    "\n",
    "Tensors are multi-dimensional arrays.\n",
    "* rank 0 for scalars\n",
    "* rank 1 for vectors\n",
    "* rank 2 for matrices\n",
    "\n",
    "Symbolic manipulation\n",
    "* build computation graphs\n",
    "* automatic and symbolic differentiation\n",
    "* send compiled expressions/graphs to CPU/GPU for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First steps with Theano\n",
    "\n",
    "http://deeplearning.net/software/theano/introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Depending on your system setup, it is typically sufficient to install Theano via\n",
    "\n",
    "    pip install Theano\n",
    "    \n",
    "For more help with the installation, please see: http://deeplearning.net/software/theano/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Introducing the TensorType variables. For a complete list, see http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(2.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define expression\n",
    "# which can be visualized as a graph\n",
    "x1 = T.scalar()\n",
    "w1 = T.scalar()\n",
    "w0 = T.scalar()\n",
    "z1 = w1 * x1 + w0\n",
    "\n",
    "# compile\n",
    "net_input = theano.function(inputs=[w1, x1, w0], outputs=z1)\n",
    "\n",
    "# execute\n",
    "answer = net_input(2.0, 1.0, 0.5)\n",
    "print(answer)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{add,no_inplace}.0\n",
      "[ 11.  17.]\n",
      "[ 12.  18.]\n"
     ]
    }
   ],
   "source": [
    "# define\n",
    "b = T.scalar('b')\n",
    "x = T.vector('x')\n",
    "W = T.matrix('W')\n",
    "\n",
    "y = x.dot(W.transpose())\n",
    "z = W.dot(x) + b\n",
    "print(z)\n",
    "# similar to python function\n",
    "# theano function can return multiple outputs\n",
    "f = theano.function(inputs = [x, W, b], outputs = [y, z])\n",
    "\n",
    "output_y, output_z = f([1, 2], [[3, 4], [5, 6]], 1)\n",
    "# output_y, output_z = f([[1, 2]], [[3, 4]], 1) # won't work as x is a vector not matrix\n",
    "# output_y, output_z = f([1, 2], [3, 4], 1) # won't work as W is a matrix not vector\n",
    "# output_y, output_z = f([1, 2], [[3, 4]], [1]) # won't work as b is a scalar not a vector/matrix\n",
    "\n",
    "print(output_y)\n",
    "print(output_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# quadratic polynomial root example\n",
    "# ax^2 + bx + c = 0\n",
    "a = T.scalar('a')\n",
    "b = T.scalar('b')\n",
    "c = T.scalar('c')\n",
    "\n",
    "core = b*b - 4*a*c\n",
    "root_p = (-b + np.sqrt(core))/(2*a)\n",
    "root_m = (-b - np.sqrt(core))/(2*a)\n",
    "\n",
    "# compile\n",
    "f = theano.function(inputs = [a, b, c], outputs = [root_p, root_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  3.]\n",
      " [ 4.  5.]]\n"
     ]
    }
   ],
   "source": [
    "a = T.matrix('a')\n",
    "b = T.matrix('b')\n",
    "\n",
    "c = a.dot(b)\n",
    "\n",
    "function = theano.function(inputs = [a,b], outputs = c)\n",
    "\n",
    "print(function([[1,0],[0,1]],[[2,3],[4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 -1.0\n",
      "4.0 3.0\n",
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "polys = [[1, 2, 1],\n",
    "         [1, -7, 12],\n",
    "         [1, 0, 1]\n",
    "        ]\n",
    "\n",
    "for poly in polys:\n",
    "    a, b, c = poly\n",
    "    root1, root2 = f(a, b, c)\n",
    "    print(root1, root2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Steps for using Theano:\n",
    "* define symbols and functions\n",
    "* compile the code\n",
    "* execute the code\n",
    "\n",
    "Each variable has a specific type (dtype)\n",
    "* trade-off between accuracy and cost (speed and storage)\n",
    "* we have to choose; good for control, bad as burden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Configuring Theano\n",
    "\n",
    "Processors:\n",
    "- Modern CPUs support 64-bit memory address.\n",
    "- GPUs (and old CPUs) remain in 32-bit.\n",
    "\n",
    "Theano supports both 32 and 64 bits.\n",
    "\n",
    "We can configure Theano to use either: float32 (for 32-bit processors) or float64 (for 64-bit processors).\n",
    "\n",
    "For more options, see\n",
    "- http://deeplearning.net/software/theano/library/config.html\n",
    "- http://deeplearning.net/software/theano/library/floatX.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# default configuration\n",
    "print(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# we can change it like this\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To change the float type globally, execute \n",
    "\n",
    "    export THEANO_FLAGS=floatX=float32 \n",
    "    \n",
    "in your bash shell. Or execute Python script as\n",
    "\n",
    "    THEANO_FLAGS=floatX=float32 python your_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Running Theano on GPU(s). For prerequisites, please see: http://deeplearning.net/software/theano/tutorial/using_gpu.html\n",
    "\n",
    "Note that `float32` is recommended for GPUs; `float64` on GPUs is currently still relatively slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THEANO_FLAGS=device='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can run a Python script on CPU (e.g. for prototyping and debug) via:\n",
    "\n",
    "    THEANO_FLAGS=device=cpu,floatX=float64 python your_script.py\n",
    "\n",
    "or GPU (e.g. for real computation) via:\n",
    "\n",
    "    THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It may also be convenient to create a `.theanorc` file in your home directory to make those configurations permanent. For example, to always use `float32`, execute\n",
    "\n",
    "    echo -e \"\\n[global]\\nfloatX=float32\\n\" >> ~/.theanorc\n",
    "    \n",
    "Or, create a `.theanorc` file manually with the following contents\n",
    "\n",
    "    [global]\n",
    "    floatX = float32\n",
    "    device = gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with array structures\n",
    "\n",
    "This is an example code to work with tensors.\n",
    "\n",
    "Create a $2 \\times 3$ tensor, and calculate its column sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column sum: [ 2.  4.  6.]\n",
      "Column sum: [ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define\n",
    "# if you are running Theano on 64 bit mode, \n",
    "# you need to use dmatrix instead of fmatrix\n",
    "x = T.matrix(name='x') # tensor with arbitrary shape\n",
    "x_sum = T.sum(x, axis=0)\n",
    "\n",
    "# compile\n",
    "calc_sum = theano.function(inputs=[x], outputs=x_sum)\n",
    "\n",
    "# execute (Python list)\n",
    "ary = [[1, 2, 3], [1, 2, 3]]\n",
    "print('Column sum:', calc_sum(ary))\n",
    "\n",
    "# execute (NumPy array)\n",
    "ary = np.array(ary, dtype=theano.config.floatX)\n",
    "print('Column sum:', calc_sum(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "<TensorType(float32, matrix)>\n",
      "<TensorType(float32, matrix)>\n"
     ]
    }
   ],
   "source": [
    "# name can help debug\n",
    "y = T.matrix(name='hello')\n",
    "z = T.matrix()\n",
    "\n",
    "print(y) # will print out variable name\n",
    "print(z) # will print out variable type\n",
    "print(y.type()) # will print out type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float32, matrix)>\n",
      "<TensorType(float64, matrix)>\n"
     ]
    }
   ],
   "source": [
    "# explicit type specification\n",
    "wf = T.fmatrix(name='wfmatrix')\n",
    "wd = T.dmatrix(name='wdmatrix')\n",
    "\n",
    "print(wf.type())\n",
    "print(wd.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory management\n",
    "\n",
    "### shared \n",
    "\n",
    "Variable with storage that is shared between functions that it appears in. \n",
    "* can have initial or constant values, e.g. weights of a neural network\n",
    "* retain value across function calls\n",
    "* cannot be used as input to a function\n",
    "* can be updated after each function call\n",
    "\n",
    "Can be more efficient than input variable\n",
    "* update in place instead of transferring around\n",
    "* Theano can then optimize the storage across CPUs and GPUs\n",
    "\n",
    "More info about memory management in Theano can be found under:\n",
    "* http://deeplearning.net/software/theano/tutorial/aliasing.html\n",
    "* https://www.quora.com/What-is-the-meaning-and-benefit-of-shared-variables-in-Theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "x = T.matrix(name='x')\n",
    "b = theano.shared(np.asarray([[1]], dtype=theano.config.floatX), name='b')\n",
    "w = theano.shared(np.asarray([[0.0, 0.0, 0.0]], \n",
    "                             dtype=theano.config.floatX))\n",
    "\n",
    "# w = w + 1.0 # this will cause error\n",
    "z = x.dot(w.T) + b\n",
    "\n",
    "update = [[w, w + 1.0]] # update w after each function call\n",
    "\n",
    "# compile\n",
    "f = theano.function(inputs=[x],\n",
    "                    updates=update,\n",
    "                    outputs=z)\n",
    "\n",
    "# won't compile as shared variable cannot be used as input\n",
    "# g = theano.function(inputs=[x, b], outputs = z)\n",
    "\n",
    "# execute\n",
    "x_data = np.array([[1, 2, 3]], dtype=theano.config.floatX)\n",
    "for i in range(5):\n",
    "    print('z%d:' % i, f(x_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### given\n",
    "\n",
    "input: transfer from CPU to GPU multiple times\n",
    "* e.g. multiple epochs\n",
    "\n",
    "shared: retained values across functions, can be updated after each function call\n",
    "* like static function variables\n",
    "* not input for function call\n",
    "* e.g. network weights\n",
    "\n",
    "given: transfer from CPU to GPU once\n",
    "* like constant variables\n",
    "* shared between multiple function calls\n",
    "* not input for function call\n",
    "* values specified, for input or shared variables, during funcion compilation \n",
    "* e.g. a mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "If we use `inputs`, a datasets is transferred from the CPU to the GPU multiple times, for example, if we iterate over a dataset multiple times (epochs) during gradient descent. \n",
    "\n",
    "We can use the `givens` variable to insert values into the graph before compiling it. Using this approach we can reduce the number of transfers from RAM (via CPUs) to GPUs to speed up learning with shared variables. \n",
    "\n",
    "Via `givens`, we can keep the dataset on the GPU if it fits (e.g., a mini-batch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define\n",
    "num_samples = 10\n",
    "samples = np.asarray([i for i in range(num_samples)],\n",
    "                     dtype=theano.config.floatX)\n",
    "\n",
    "# samples = theano.shared(samples)\n",
    "\n",
    "x = T.lscalar(name='index')\n",
    "#y = theano.shared(np.asscalar(np.array([1], dtype=theano.config.floatX)))\n",
    "y = T.vector(name='samples')\n",
    "w = theano.shared(np.asscalar(np.array([0], dtype=theano.config.floatX)))\n",
    "\n",
    "z = y[x]*w\n",
    "\n",
    "# compile\n",
    "f = theano.function(inputs = [x],\n",
    "                    updates = [[w, w+1]],\n",
    "                    givens = {y: samples},\n",
    "                    outputs = z)\n",
    "\n",
    "# run\n",
    "for i in range(np.prod(samples.shape)):\n",
    "    print(f(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "x_data = np.array([[1, 2, 3]], dtype=theano.config.floatX)\n",
    "\n",
    "x = T.matrix(name='hi')\n",
    "w = theano.shared(np.asarray([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=theano.config.floatX))\n",
    " \n",
    "# an input variable can be given\n",
    "b_data = np.array([[-1, 0, 1]], dtype=theano.config.floatX)\n",
    "b = T.matrix(name='bias')\n",
    "\n",
    "# a shared variable can be given\n",
    "c_data = np.array([[4, 5, 6]], dtype=theano.config.floatX)\n",
    "c = theano.shared(np.asarray([[0]], dtype=theano.config.floatX))\n",
    "\n",
    "z = x.dot(w.T) + b + c\n",
    "\n",
    "updates = [[w, w + 1.0]]\n",
    "givens = {b: b_data, c: c_data}\n",
    "\n",
    "# compile\n",
    "net_input = theano.function(inputs=[x], \n",
    "                            updates=updates, \n",
    "                            givens=givens,\n",
    "                            outputs=z)\n",
    "\n",
    "# execute\n",
    "for i in range(5):\n",
    "    print('z:', net_input(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapping things up: A  linear regression example\n",
    "\n",
    "Model:\n",
    "$\n",
    "y = \\sum_{i=0}^n w_i x_i = \\mathbf{w}^T \\mathbf{x}\n",
    "$\n",
    "with $x_0 = 1$.\n",
    "\n",
    "Given a collection of sample data $\\{\\mathbf{x^{(i)}}, y^{(i)} \\}$, find the line $\\mathbf{w}$ that minimizes the regression error:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(X, Y, \\mathbf{w}) \n",
    "= \\sum_i \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2 \n",
    "= \\sum_i \\left( y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} \\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### 2D case\n",
    "\n",
    "$\n",
    "y = w_0 + w_1 x\n",
    "$\n",
    "\n",
    "<img src='./images/10_01.png' width=90%> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.asarray([[0.0], [1.0], [2.0], [3.0], [4.0],\n",
    "                      [5.0], [6.0], [7.0], [8.0], [9.0]], \n",
    "                     dtype=theano.config.floatX)\n",
    "\n",
    "y_train = np.asarray([1.0, 1.3, 3.1, 2.0, 5.0, \n",
    "                      6.3, 6.6, 7.4, 8.0, 9.0], \n",
    "                     dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Implement the training function\n",
    "\n",
    "Notice:\n",
    "* the symbolic differentiation for the gradient part\n",
    "* how different variable types (input, shared, givens, output) are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "\n",
    "def train_linreg(X_train, y_train, eta, epochs):\n",
    "\n",
    "    costs = []\n",
    "    # Initialize arrays\n",
    "    eta0 = T.scalar('eta0') # learning rate\n",
    "    y = T.vector(name='y') \n",
    "    X = T.matrix(name='X')   \n",
    "    w = theano.shared(np.zeros(\n",
    "                      shape=(X_train.shape[1] + 1),\n",
    "                      dtype=theano.config.floatX),\n",
    "                      name='w')\n",
    "    \n",
    "    # calculate cost\n",
    "    y_pred = T.dot(X, w[1:]) + w[0]\n",
    "    errors = y - y_pred\n",
    "    cost = T.sum(T.pow(errors, 2)) \n",
    "\n",
    "    # perform gradient update\n",
    "    gradient = T.grad(cost, wrt=w) # symbolic differentialtion\n",
    "    update = [(w, w - eta0 * gradient)]\n",
    "\n",
    "    # compile model\n",
    "    train = theano.function(inputs=[eta0],\n",
    "                            outputs=cost,\n",
    "                            updates=update,\n",
    "                            givens={X: X_train,\n",
    "                                    y: y_train})      \n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # since eta is input\n",
    "        # we can gradually change the learning rate\n",
    "        costs.append(train(eta))\n",
    "    \n",
    "    return costs, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Plotting the sum of squared errors cost vs epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "costs, w = train_linreg(X_train, y_train, eta=0.001, epochs=10)\n",
    "   \n",
    "plt.plot(range(1, len(costs)+1), costs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/cost_convergence.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_linreg(X, w):\n",
    "    Xt = T.matrix(name='X')\n",
    "    y_pred = T.dot(Xt, w[1:]) + w[0]\n",
    "    predict = theano.function(inputs=[Xt], givens={w: w}, outputs=y_pred)\n",
    "    return predict(X)\n",
    "\n",
    "plt.scatter(X_train, y_train, marker='s', s=50)\n",
    "plt.plot(range(X_train.shape[0]), \n",
    "         predict_linreg(X_train, w), \n",
    "         color='gray', \n",
    "         marker='o', \n",
    "         markersize=4, \n",
    "         linewidth=3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/linreg.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theano for neural networks\n",
    "\n",
    "Also use Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Choosing activation functions for feedforward neural networks\n",
    "\n",
    "There are various activation functions for a multi-layer neural networks.\n",
    "* in theory we can use any differential function\n",
    "* in practice we want (1) non-linearity and (2) goood convergence for gradient descent\n",
    "\n",
    "Sigmoid is one we have seen.\n",
    "* mimics biological neurons\n",
    "* converge slowly for deep networks (vanishing gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logistic function recap\n",
    "\n",
    "The logistic function, often just called \"sigmoid function\" is in fact a special case of a sigmoid function.\n",
    "\n",
    "Linear input $z$:\n",
    "$$\n",
    "\\begin{align}\n",
    "z &=  w_0x_{0} + \\dots + w_mx_{m} \n",
    "\\\\\n",
    "&= \\sum_{j=0}^{m} x_{j}w_{j} \n",
    "\\\\ \n",
    "&= \\mathbf{w}^T\\mathbf{x}\n",
    "\\end{align}\n",
    "$$\n",
    "$w_0$ is the bias term, matching $x_0 = 1$\n",
    "\n",
    "Logistic activation function:\n",
    "\n",
    "$$\\phi_{logistic}(z) = \\frac{1}{1 +  e^{-z}}$$\n",
    "\n",
    "Output range: (0, 1)\n",
    "* probability for the positive class $z > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Concrete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# note that first element (X[0] = 1) to denote bias unit\n",
    "\n",
    "X = np.array([[1, 1.4, 1.5]])\n",
    "w = np.array([0.0, 0.2, 0.4])\n",
    "\n",
    "def net_input(X, w):\n",
    "    z = X.dot(w)\n",
    "    return z\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return np.asscalar(logistic(z))\n",
    "\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple outputs\n",
    "\n",
    "One-hot encoding for multi-class classification.\n",
    "* $K$ outputs for $K$ classes\n",
    "\n",
    "Logistic activation outputs cannot be directly interpreted as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "A MLP perceptron with \n",
    "* 3 hidden units + 1 bias unit in the hidden unit \n",
    "* 3 output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# W : array, shape = [n_output_units, n_hidden_units+1]\n",
    "#          Weight matrix for hidden layer -> output layer.\n",
    "# note that first column (A[:][0] = 1) are the bias units\n",
    "W = np.array([[1.1, 1.2, 1.3, 0.5],\n",
    "              [0.1, 0.2, 0.4, 0.1],\n",
    "              [0.2, 0.5, 2.1, 1.9]])\n",
    "\n",
    "# A : array, shape = [n_hidden+1, n_samples]\n",
    "#          Activation of hidden layer.\n",
    "# note that first element (A[0][0] = 1) is for the bias units\n",
    "\n",
    "A = np.array([[1.0], \n",
    "              [0.1], \n",
    "              [0.3], \n",
    "              [0.7]])\n",
    "\n",
    "# Z : array, shape = [n_output_units, n_samples]\n",
    "#          Net input of output layer.\n",
    "\n",
    "Z = W.dot(A) \n",
    "y_probas = logistic(Z)\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The outputs do not sum to 1 and thus are not probabilities.\n",
    "\n",
    "Need normalization for probability\n",
    "* divide all outputs by their summation\n",
    "* softmax which should be applied to z (linear inputs) before logistic activation\n",
    "\n",
    "OK for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print('predicted class label: %d' % y_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimating probabilities in multi-class classification via the softmax function\n",
    "\n",
    "The softmax function \n",
    "* is a generalization of the logistic function\n",
    "* allows us to compute meaningful class probabilities in multi-class settings\n",
    "* i.e. multinomial logistic regression\n",
    "\n",
    "$z$: linear input as usual\n",
    "\n",
    "$K$: number of classes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y=j|z) =\\phi_{softmax}(z) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$P(y=j | z)$: probability of class $j$ for input $z$ in range $(0, 1)$ \n",
    "\n",
    "<img src=\"./images/bonus_softmax_1.png\" width=100%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "def softmax_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_probas = softmax(Z) # same Z computed above\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_probas.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The class probabilities sum to 1.\n",
    "\n",
    "The predicted class is the same as logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "y_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadening the output spectrum using a hyperbolic tangent\n",
    "\n",
    "Another special case of a sigmoid function, it can be interpreted as a rescaled version of the logistic function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{tanh}(z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\phi_{tanh}$ is a rescaled version of $\\phi_{logistic}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{tanh}(z) = 2 \\phi_{logistic}(z) - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Output range: (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    e_p = np.exp(z) \n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "# alternatives:\n",
    "# from scipy.special import expit\n",
    "# log_act = expit(z)\n",
    "# tanh_act = np.tanh(z)\n",
    "\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel('net input $z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(-1, color='black', linestyle='--')\n",
    "\n",
    "plt.plot(z, tanh_act, \n",
    "         linewidth=2, \n",
    "         color='black', \n",
    "         label='tanh')\n",
    "plt.plot(z, log_act, \n",
    "         linewidth=2, \n",
    "         color='lightgreen', \n",
    "         label='logistic')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/activation.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different activation functions\n",
    "\n",
    "<img src='./images/13_05.png' width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training neural networks efficiently using Keras\n",
    "\n",
    "A library (stared in early 2015) to facilitate neural network training.\n",
    "* built on top of Theano\n",
    "* intuitive and popular API\n",
    "* front-end for Theano and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Once you have Theano installed, [Keras](https://github.com/fchollet/keras) can be installed via\n",
    "\n",
    "    pip install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Download the 4 MNIST datasets from http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "- train-images-idx3-ubyte.gz:  training set images (9912422 bytes) \n",
    "- train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) \n",
    "- t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) \n",
    "- t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "\n",
    "2) Unzip those files\n",
    "\n",
    "3) Copy the unzipped files to a directory `./mnist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Li-Yi: I enhanced the functions below so that we can do everything (including downloading and decompression) inside ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import struct\n",
    "import gzip\n",
    "import numpy as np\n",
    " \n",
    "def open_mnist(full_path):\n",
    "    if full_path.find('.gz') >= 0:\n",
    "        return gzip.open(full_path, 'rb')\n",
    "    else:\n",
    "        return open(full_path, 'rb')\n",
    "        \n",
    "def pick_mnist(path, name, exts):\n",
    "    for ext in exts:\n",
    "        full_path = os.path.join(path, name + ext)\n",
    "        if os.path.isfile(full_path):\n",
    "            return full_path\n",
    "    # none of the exts options works    \n",
    "    return None\n",
    "\n",
    "def load_mnist(path, kind='train', exts=['', '.gz']):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = pick_mnist(path, kind + '-labels-idx1-ubyte', exts)\n",
    "    images_path = pick_mnist(path, kind + '-images-idx3-ubyte', exts)\n",
    "    \n",
    "    with open_mnist(labels_path) as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))        \n",
    "        if(magic != 2049):\n",
    "            raise IOError(str(magic) + ' != ' + str(2049))\n",
    "            \n",
    "        # np.fromfile does not work with gzip open   \n",
    "        # http://stackoverflow.com/questions/15966335/efficient-numpy-fromfile-on-zipped-files\n",
    "        # labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        content = lbpath.read()    \n",
    "        labels = np.frombuffer(content, dtype=np.uint8)\n",
    "        if(len(labels) != n):\n",
    "            raise IOError(str(len(labels)) + ' != ' + str(n))\n",
    "        \n",
    "    with open_mnist(images_path) as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        if(magic != 2051):\n",
    "            raise IOError(str(magic) + ' != ' + str(2051))\n",
    "            \n",
    "        # images = np.fromfile(imgpath, dtype=np.uint8).reshape(num, rows*cols)\n",
    "        content = imgpath.read()    \n",
    "        images = np.frombuffer(content, dtype=np.uint8).reshape(num, rows*cols)\n",
    "        if(num != len(labels)):\n",
    "            raise IOError(str(num) + ' != ' + str(len(labels)))\n",
    "            \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist_data_folder = os.path.join('..', 'datasets', 'mnist')\n",
    "exts = ['', '.gz'] # for already gunzipped files and not yet gzipped files\n",
    "\n",
    "X_train, y_train = load_mnist(mnist_data_folder, kind='train', exts=exts)\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist(mnist_data_folder, kind='t10k', exts=exts)\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-layer Perceptron in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to run the following code via GPU, you can execute the Python script that was placed in this directory via\n",
    "\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_keras_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import theano \n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "X_train = X_train.astype(theano.config.floatX)\n",
    "X_test = X_test.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### One-hot encoding of the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('First 3 labels: ', y_train[:3])\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(y_train) \n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_ohe[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Implement a neural network\n",
    "\n",
    "* fully connected with 2 hidden layers\n",
    "\n",
    "* tanh for hidden layers\n",
    "\n",
    "* softmax for output layer\n",
    "\n",
    "* cross entropy loss function (to match softmax output)\n",
    "\n",
    "* SGD optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "np.random.seed(1) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=X_train.shape[1], \n",
    "                output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh'))\n",
    "\n",
    "model.add(Dense(output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh'))\n",
    "\n",
    "model.add(Dense(output_dim=y_train_ohe.shape[1], \n",
    "                init='uniform', \n",
    "                activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train_ohe, \n",
    "          nb_epoch=50, \n",
    "          batch_size=300, \n",
    "          verbose=1, \n",
    "          validation_split=0.1 # 10% of training data for validation per epoch\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(X_test, verbose=0)\n",
    "test_acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "Plenty deep learning libraries to use so that we do not have to write code from scratch.\n",
    "* Theano: Python interface, resource management (processors and memory)\n",
    "* TensorFlow: similar flavor to Theano, developed by Google\n",
    "* Keras: front-end for Theano and TensorFlow, easy and intuitive API\n",
    "* Torch7: via lua language, used by DeepMind folks and Yann LeCun\n",
    "* Caffe: c++ library\n",
    "* etc.\n",
    "\n",
    "Alternative libraries with Theano:\n",
    "* Pylearn2\n",
    "* Lasagne: more minimalistic and extensible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading\n",
    "\n",
    "* PML Chapter 13"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
