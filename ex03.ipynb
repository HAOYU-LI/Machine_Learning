{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 3 - basic classifiers\n",
    "\n",
    "Math practice and coding application for main classifiers introduced in Chapter 3 of the Python machine learning book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weighting\n",
    "\n",
    "Note that this assignment is more difficult than the previous ones, and thus has a higher weighting 3 and longer duration (3 weeks). Each one of the previous two assignments has a weighting 1.\n",
    "\n",
    "Specifically, the first 3 assignments contribute to your continuous assessment as follows:\n",
    "\n",
    "Assignment weights: $w_1 = 1, w_2 = 1, w_3 = 3$\n",
    "\n",
    "Assignment grades: $g_1, g_2, g_3$\n",
    "\n",
    "Weighted average: $\\frac{1}{\\sum_i w_i} \\times \\sum_i \\left(w_i \\times g_i \\right)$\n",
    "\n",
    "Future assignments will be added analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RBF kernel (20 points)\n",
    "\n",
    "Show that a Gaussian RBF kernel can be expressed as a dot product:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= \\phi(\\mathbf{x})^T \\phi(\\mathbf{y})\n",
    "$$\n",
    "by spelling out the mapping function $\\phi$.\n",
    "\n",
    "For simplicity\n",
    "* you can assume both $\\mathbf{x}$ and $\\mathbf{y}$ are 2D vectors\n",
    "$\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    ", \\;\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "* we use a scalar unit variance here\n",
    "\n",
    "even though the proof can be extended for vectors $\\mathbf{x}$ $\\mathbf{y}$ and general covariance matrices.\n",
    "\n",
    "Hint: use Taylor series expansion of the exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "\\begin{align}\n",
    "K(\\mathbf{x}, \\mathbf{y})\n",
    "&=\n",
    "\\large e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2}\n",
    "\\\\\n",
    "&= \n",
    "\\large e^\\frac{-|(x1-x2)^2+(x2-y2)^2|}{2}\n",
    "\\\\\n",
    "&= \n",
    "\\large e^{(-\\frac{x1^2+x2^2}{2}-\\frac{y1^2+y2^2}{2}+x1*y1+x2*y2)}\n",
    "\\\\\n",
    "&=\n",
    "\\large e^{(\\frac{||x||^2}{-2})}*e^{(\\frac{||y||^2}{-2})}*e^{(X^T*Y))}\n",
    "\\\\\n",
    "&=\n",
    "\\large e^{(\\frac{||x||^2}{-2})} * e^{(\\frac{||y||^2}{-2})}*\\sum_{n=1}^\\infty(\\frac{(X^TY)^n}{n!})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, we need to define a vector notation :\n",
    "$$\n",
    "S_{x,n}^T\\ =\\ [\\sqrt{_nC_n}x_1^nx_2^0,\\sqrt{_{n-1}C_n}x_1^{n-1}x_2^1,...,\\sqrt{_0C_n}x_1^0x_2^n]\n",
    "$$\n",
    "\n",
    "For example:\n",
    "$$\n",
    "S_{x,1}^T\\ =\\ [\\sqrt{1}x_1^1,\\sqrt{1}x_2^1 ]\n",
    "\\\\\n",
    "S_{x,2}^T\\ =\\ [\\sqrt{1}x_1^2,\\sqrt{2}x_1^1x_2^1,\\sqrt{1}x_2^2 ]\n",
    "$$\n",
    "\n",
    "Then the mapping function has infinite parts, which is:\n",
    "\n",
    "$$\n",
    "\\phi(x)^T=(e^(\\frac{||x||^2}{-2})*\\frac{S_{x,1}^T}{\\sqrt{(1!)}} , e^(\\frac{||x||^2}{-2})*\\frac{S_{x,2}^T}{\\sqrt{(2!)}} , e^(\\frac{||x||^2}{-2})*\\frac{S_{x,3}^T}{\\sqrt{(3!)}}....)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi(y)=(e^(\\frac{||y||^2}{-2})*\\frac{S_{y,1}}{\\sqrt{(1!)}} , e^(\\frac{||y||^2}{-2})*\\frac{S_{y,2}}{\\sqrt{(2!)}} , e^(\\frac{||y||^2}{-2})*\\frac{S_{x,3}}{\\sqrt{(3!)}}....)^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel SVM complexity (10 points)\n",
    "\n",
    "How would the complexity (or number of parameters) of a kernel SVM change with the amount of training data, and why?\n",
    "Note that the answer may depend on the specific kernel used.\n",
    "Consider specifically the following types of kernels $K(\\mathbf{x}, \\mathbf{y})$.\n",
    "* linear:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}\n",
    "$$\n",
    "* polynomial with degree $q$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "(\\mathbf{x}^T\\mathbf{y} + 1)^q\n",
    "$$\n",
    "* RBF with distance function $D$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "$\\large Linear\\ kernel\\ function$:\n",
    "\n",
    "    For linear kernel function, the number of parameters is fixed and it will not change with the \n",
    "\n",
    "amount of the training data. However, the value of parameters may change according to the input data but the \n",
    "\n",
    "dimensions or the number of parameters will not.\n",
    "\n",
    "\n",
    "\n",
    "$\\large Polynomial\\ with\\ degree\\ q$: \n",
    "    \n",
    "    As polynomial functions are essentially mapping the points into higher dimensional space so \n",
    "\n",
    "that the points can be separated by a hyperplane. The dimensions of the mapping plane is predetermined by selection of \n",
    "\n",
    "degree q. So the dimensions or number of parameters will not change as well. Below are mathematical explanations:\n",
    "\n",
    "Let's assume that the value 1 of below formula is the product of the $X_{dimension+1}Y_{dimension+1}$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "K(X,Y) = (X^TY+1)^q=(\\sum_{i=1}^{d+1}X_iY_i)^q = \\sum_{j_1+j_2+...+j_{d+1}=q}{\\frac{q!}{j_1!j_2!*...*j_{d+1}!}\\prod_{i=1}^{d+1}(X_iY_i)^{j_i}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then we can tell from from the kernel function that the dimensions got from this function is \n",
    "\n",
    "$$\\mathbb{R}^{{d+q \\choose q}} = \\mathbb{R}^{\\frac{(d+q)!}{d!q!}}$$\n",
    "\n",
    "which means the dimensions related to number of features and q and is predetermined.\n",
    "\n",
    "\n",
    "\n",
    "$\\large RBF\\ with\\ distance\\ function\\ D$: \n",
    "    \n",
    "    support vector machine using RBF as kernel function may have infinite dimension mapping \n",
    "\n",
    "space, since the taylor expansion of exponential function is infinite. It's a little bit like polynomial function with\n",
    "\n",
    "infinite degree and if we don't choose the features properly, it may result in that the complexity or the number of \n",
    "\n",
    "parameters is too high. So under the situation with inappropriate features chosen, the complexity will become higher\n",
    "\n",
    "when the data size becomes larger and larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian density Bayes (30 points)\n",
    "\n",
    "$$\n",
    "p\\left(\\Theta | \\mathbf{X}\\right)\n",
    "= \n",
    "\\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "$$\n",
    "\n",
    "Assume both the likelihood and prior have Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "p(\\Theta)\n",
    "&=\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive $\\Theta$ from the dataset $\\mathbf{X}$ via the following methods:\n",
    "\n",
    "### ML (maximum likelihood) estimation \n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{Bayes} \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \n",
    "\\int \\Theta p(\\Theta | \\mathbf{X}) d\\Theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\Large Solution\\ to\\ Gaussian\\ density\\ Bayes:$\n",
    "\n",
    "\n",
    "$\\large We\\ first\\ find\\ the\\ MLE:$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "log(P(\\mathbf{X} | \\Theta)) \n",
    "&=\n",
    "-log((2pi)^{N/2}*sigma^N) - \\frac{ \\sum((X^{(t)}-theta)^2) }{2*sigma^2}\n",
    "\\\\\n",
    "\\frac{\\phi}{\\phi \\theta}log(P(x|\\theta))\n",
    "&=\n",
    "\\frac{\\sum(X^{(t)})}{\\sigma^2}- \\frac{N\\theta}{\\sigma^2}\n",
    "\\\\\n",
    "\\frac{\\phi}{\\phi \\theta}log(P(x|\\theta)) \n",
    "&= \n",
    "0\n",
    "\\\\\n",
    "\\frac{1}{N}*\\sum_{t=1}^{N}(x^{(t)})\n",
    "&=\n",
    "\\theta(MLE)\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\large Next,\\ we\\ derive\\ the\\ map\\ estimation:$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(X|\\theta)*p(\\theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) * \\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\\\\n",
    "log(p(X|\\theta)*p(\\theta))\n",
    "&=\n",
    "-log((2\\pi)^{\\frac{N}{2}}*\\sigma^N) - \\frac{\\sum_{t=1}^{N}((x^{(t)}-\\theta)^2)}{2\\sigma^2} - log((2\\pi)^{0.5}\\sigma_0) - \\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2}\n",
    "\\\\\n",
    "\\frac{\\phi}{\\phi \\theta}log(p(X|\\theta)*p(\\theta))\n",
    "&=\n",
    "\\frac{\\sum_{t=1}^{N}(x^{(t)})}{\\sigma^2} - \\frac{N\\theta}{\\sigma^2} - 2(\\theta - \\mu_0)\n",
    "\\\\\n",
    "\\frac{\\phi}{\\phi \\theta}log(p(X|\\theta)*p(\\theta))\n",
    "&=\n",
    "0\n",
    "\\\\\n",
    "\\frac{\\sigma_0^2\\sum_{t=1}^{N}(x^{(t)})+ \\sigma^2*\\mu_0}{N\\sigma_0^2+\\sigma^2}\n",
    "&= \n",
    "\\theta(MAP) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\large Finally,\\ we\\ will\\ derivate\\ the\\ Bayes\\ estimation:$\n",
    "\n",
    "To derivate Bayes estimation, we need obtain two points that :\n",
    "\n",
    "The graph of normal dsitribution arrives at maximum point when $\\theta = \\mu$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta|x)\n",
    "&\\propto\n",
    "p(x|\\theta)p(\\theta)\n",
    "\\\\\n",
    "&\\propto\n",
    "exp(-\\frac{1}{2\\sigma^2}*\\sum_{t=1}^{N}((x^{(t)}-\\theta)^2)) * exp(-\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2})\n",
    "\\\\\n",
    "&=\n",
    "exp(\\frac{N\\sigma^2+N(\\theta-\\frac{\\sum_{t=1}^{N}(x^{(t)})}{N})^2}{-2\\sigma^2}+\\frac{(\\theta-\\mu_0)^2}{-2\\sigma_0^2})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This satisfies the normal distribution with mean, let's say, $\\mu_N$\n",
    "\n",
    "And we can get the mean of it when the function gets its maximum value.\n",
    "\n",
    "We then take derivative of its log function to find the maximum point.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "log(p(\\theta|x))\n",
    "&\\propto\n",
    "\\frac{N\\sigma^2+N(\\theta-\\frac{\\sum_{t=1}^{N}(x^{(t)})}{N})^2}{-2\\sigma^2}+\\frac{(\\theta-\\mu_0)^2}{-2\\sigma_0^2}\n",
    "\\\\\n",
    "\\frac{\\phi}{\\phi \\theta} log(p(\\theta|x))\n",
    "&\\propto\n",
    "\\frac{N\\theta-\\sum_{t=1}^{N}(x^{(t)})}{-\\sigma^2} + \\frac{(\\theta-\\mu_0)}{-\\sigma_0^2}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\frac{N\\theta-\\sum_{t=1}^{N}(x^{(t)})}{-\\sigma^2} + \\frac{(\\theta-\\mu_0)}{-\\sigma_0^2}\n",
    "\\\\\n",
    "\\frac{\\sigma_0^2\\sum_{t=1}^{N}(x^{(t)})+ \\sigma^2*\\mu_0}{N\\sigma_0^2+\\sigma^2}\n",
    "&=\n",
    "\\theta(Bayes)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#<img src = \"./images/MLE.jpg\">\n",
    "#<img src = \"./images/MAP.jpg\">\n",
    "#<img src = \"./images/Bayes.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-written digit classification (40 points)\n",
    "\n",
    "In the textbook sample code we applied different scikit-learn classifers for the Iris data set.\n",
    "\n",
    "In this exercise, we will apply the same set of classifiers over a different data set: hand-written digits.\n",
    "Please write down the code for different classifiers, choose their hyper-parameters, and compare their performance via the accuracy score as in the Iris dataset.\n",
    "Which classifier(s) perform(s) the best and worst, and why?\n",
    "\n",
    "The classifiers include:\n",
    "* perceptron\n",
    "* logistic regression\n",
    "* SVM\n",
    "* decision tree\n",
    "* random forest\n",
    "* KNN\n",
    "* naive Bayes\n",
    "\n",
    "The dataset is available as part of scikit learn, as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "X is : [  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.   5.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # training data\n",
    "y = digits.target # training label\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print (\"X is :\",X[0,0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf1c9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAEHCAYAAAByadD+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npP4L2rCV5RRwE1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2d92e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "index = 4\n",
    "pl.gray()\n",
    "pl.matshow(digits.images[index])\n",
    "pl.title('digit ' + str(digits.target[index]))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Date Preprocessing\n",
    "Hint: How you divide training and test data set? And apply other techinques we have learned if needed.\n",
    "You could take a look at the Iris data set case in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1257, test: 540\n"
     ]
    }
   ],
   "source": [
    "#Your code comes here\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# splite data into 70% training and 30% test:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "num_training = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "print(\"training: \" + str(num_training) + \", test: \"+ str(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data scaling:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to return the accuracy score:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def printAccuracy(testData,predData):\n",
    "    print(\"The misclassified samples: %d out of %d\" % ((predData!=testData).sum(),testData.shape[0]))\n",
    "    print(\"Accuracy: %.3f\"%accuracy_score(testData,predData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassified samples: 38 out of 540\n",
      "Accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(n_iter = 200, eta0 = 0.001,random_state = 0)\n",
    "_ = ppn.fit(X_train,y_train)\n",
    "\n",
    "#y_pred = ppn.predict(X_test)\n",
    "#printAccuracy(y_test,y_pred)\n",
    "\n",
    "#if we use the standardized data:\n",
    "_ = ppn.fit(X_train_std,y_train)\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "printAccuracy(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.935185185185\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C = 1000.0, random_state = 0)\n",
    "#lr.fit(X_train, y_train)\n",
    "#y_pred = lr.predict(X_test)\n",
    "\n",
    "#Similarly, if we use the standardized data:\n",
    "lr.fit(X_train_std, y_train)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print(\"Accuracy score: \",accuracy_score(y_test,y_pred))\n",
    "\n",
    "#print(lr.predict_proba(X_test[0,:].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97037037037\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "# Using linear kernel function:\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1.0, random_state = 0)\n",
    "#svm.fit(X_train,y_train)\n",
    "#y_pred = svm.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "svm.fit(X_train_std,y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Accuracy :\",accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.948148148148\n"
     ]
    }
   ],
   "source": [
    "#We now use the radial basis function to train our model\n",
    "svm = SVC(kernel = 'rbf',random_state = 0,gamma = 0.1, C = 10.0)\n",
    "#svm.fit(X_train,y_train)\n",
    "#y_pred = svm.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "svm.fit(X_train_std,y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Accuracy :\",accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97037037037\n"
     ]
    }
   ],
   "source": [
    "#How about sigmoid:\n",
    "svm = SVC(kernel = 'sigmoid',random_state = 0,gamma = 0.009, C = 3.0)\n",
    "#svm.fit(X_train,y_train)\n",
    "#y_pred = svm.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "svm.fit(X_train_std,y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Accuracy :\",accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.864814814815\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "#We use entropy to calculate our Information\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 10,random_state = 0)\n",
    "#tree.fit(X_train,y_train)\n",
    "#y_pred = tree.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data :\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "tree.fit(X_train_std,y_train)\n",
    "y_pred = tree.predict(X_test_std)\n",
    "print(\"Accuracy :\",accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifer #5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97037037037\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion = 'entropy',\n",
    "                                n_estimators = 100,\n",
    "                                random_state = 3,\n",
    "                                n_jobs = 2)\n",
    "#forest.fit(X_train,y_train)\n",
    "#y_pred = forest.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data :\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "forest.fit(X_train_std,y_train)\n",
    "y_pred = forest.predict(X_test_std)\n",
    "print(\"Accuracy :\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.974074074074\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1,p = 2,metric = 'minkowski')\n",
    "#knn.fit(X_train,y_train)\n",
    "#y_pred = knn.predict(X_test)\n",
    "#print(\"Accuracy for non_standatdized data: \",accuracy_score(y_test,y_pred))\n",
    "\n",
    "knn.fit(X_train_std,y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print(\"Accuracy : \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For knn, it's interesting that standardized data gives negative effect to the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.768518518519\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#gnb = GaussianNB()\n",
    "#gnb.fit(X_train,y_train)\n",
    "#y_pred = gnb.predict(X_test)\n",
    "#print(\"Accuracy for non-standardized data: \",accuracy_score(y_test,y_pred))\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_std,y_train)\n",
    "y_pred = gnb.predict(X_test_std)\n",
    "print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conclusion:\n",
    "\n",
    "It seems that KNN has the best performance while Naive Bayes has the worst performance. There are reasons for this outcome. \n",
    "\n",
    "$\\large KNN: $\n",
    "\n",
    "For K-nearest neighbor algorithm, There are quite a lot of advantages in using it under our question assumption.\n",
    "The training dataset and test dataset have relatively small size of scale. As we know the features of KNN, small scale of dataset with efficient data structure(In scikit learn, \"ball tree\",\"KDtree\",\"brute force\" will be automatically determined to be used) will help improve computational complexity. The outcome of this supervised non-parametric learning algorithm will give us a relatively precise prediction for this non-linear separable dataset. Besides, the decision tree has a good performance for similar reason.\n",
    "\n",
    "$\\large Naive\\ Bayes: $\n",
    "\n",
    "If we need Naive Bayes to be efficient and accurate, we need the assumption that different features are independent and the data is linearly seperable to be true. However, in the classification of hand-written digits, different features are not mutually independent since some pixels(features) will often be highlighted at the same time once a digit is written. Besides, the 10 classes aren't linearly separable obviously and as a result of the above, the Naive Bayes isn't a suitable algorithm for hand written digits recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
